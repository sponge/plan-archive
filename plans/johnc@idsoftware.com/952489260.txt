Welcome to id Software's Finger Service V1.5!

Name: John Carmack
Email: johnc@idsoftware.com
Description: Programmer
Project: Quake 3 Arena
Last Updated: 03/07/2000 23:47:56 (Central Standard Time)
-------------------------------------------------------------------------------
3/7/00
------
This is something I have been preaching for a couple years, but I 
finally got around to setting all the issues down in writing.

First, the statement:

Virtualized video card local memory is The Right Thing.

Now, the argument (and a whole bunch of tertiary information):

If you had all the texture density in the world, how much texture 
memory would be needed on each frame?

For directly viewed textures, mip mapping keeps the amount of 
referenced texels between one and one quarter of the drawn pixels.  
When anisotropic viewing angles and upper level clamping are taken into 
account, the number gets smaller.  Take 1/3 as a conservative estimate.

Given a fairly aggressive six texture passes over the entire screen, 
that equates to needing twice as many texels as pixels.  At 1024x768 
resolution, well under two million texels will be referenced, no matter 
what the finest level of detail is.  This is the worst case, assuming 
completely unique texturing with no repeating.  More commonly, less 
than one million texels are actually needed.

As anyone who has tried to run certain Quake 3 levels in high quality 
texture mode on an eight or sixteen meg card knows, it doesnt work out 
that way in practice.  There is a fixable part and some more 
fundamental parts to the fall-over-dead-with-too-many-textures problem.

The fixable part is that almost all drivers perform pure LRU (least 
recently used) memory management.  This works correctly as long as the 
total amount of textures needed for a given frame fits in the cards 
memory after they have been loaded.  As soon as you need a tiny bit 
more memory than fits on the card, you fall off of a performance cliff.  
If you need 14 megs of textures to render a frame, and your graphics 
card has 12 megs available after its frame buffers, you wind up loading 
14 megs of texture data over the bus every frame, instead of just the 2 
megs that dont fit.  Having the cpu generate 14 megs of command 
traffic can drop you way into the single digit frame rates on most 
drivers.

If an application makes reasonable effort to group rendering by 
texture, and there is some degree of coherence in the order of texture 
references between frames, much better performance can be gotten with a 
swapping algorithm that changes its behavior instead of going into a 
full thrash:

While ( memory allocation for new texture fails ) 
Find the least recently used texture.
If the LRU texture was not needed in the previous frame,
	Free it
Else
	Free the most recently used texture that isnt bound to an 
active texture unit

Freeing the MRU texture seems counterintuitive, but what it does is 
cause the driver to use the last bit of memory as a sort of scratchpad 
that gets constantly overwritten when there isnt enough space.  Pure 
LRU plows over all the other textures that are very likely going to be 
needed at the beginning of the next frame, which will then plow over 
all the textures that were loaded on top of them.

If an application uses textures in a completely random order, any given 
replacement policy has the some effect

Texture priority for swapping is a non-feature.  There is NO benefit to 
attempting to statically prioritize textures for swapping.  Either a 
texture is going to be referenced in the next frame, or it isnt.  
There arent any useful gradations in between.  The only hint that 
would be useful would be a notice that a given texture is not going to 
be in the next frame, and that just doesnt come up very often or cover 
very many texels.

With the MRU-on-thrash texture swapping policy, things degrade 
gracefully as the total amount of textures increase but due to several 
issues, the total amount of textures calculated and swapped is far 
larger than the actual amount of texels referenced to draw pixels.

The primary problem is that textures are loaded as a complete unit, 
from the smallest mip map level all the way up to potentially a 2048 by 
2048 top level image.  Even if you are only seeing 16 pixels of it off 
in the distance, the entire 12 meg stack might need to be loaded.

Packing can also cause some amount of wasted texture memory.  When you 
want to load a two meg texture, it is likely going to require a lot 
more than just two megs of free texture memory, because a lot of it is 
going to be scattered around in 8k to 64k blocks.  At the pathological 
limit, this can waste half your texture memory, but more reasonably it 
is only going to be 10% or so, and cause a few extra texture swap outs.

On a frame at a time basis, there are often significant amounts of 
texels even in referenced mip levels that are not seen.  The back sides 
of characters, and large textures on floors can often have less than 
50% of their texels used during a frame.  This is only an issue as they 
are being swapped in, because they will very likely be needed within 
the next few frames.  The result is one big hitch instead of a steady 
loading.

There are schemes that can help with these problems, but they have 
costs.

Packing losses can be addressed with compaction, but that has rarely 
proven to be worthwhile in the history of memory management.  A 128-bit 
graphics accelerator could compact and sort 10 megs of texture memory 
in about 10 msec if desired.

The problems with large textures can be solved by just not using large 
textures.  Both packing losses, and non- referenced texels can be 
reduced by chopping everything up into 64x64 or 128x128 textures.  This 
requires preprocessing, adds geometry, and requires messy overlap of 
the textures to avoid seaming problems.

It is possible to estimate which mip levels will actually be needed and 
only swap those in.   An application cant calculate exactly the mip 
map levels that will be referenced by the hardware, because there are 
slight variations between chips and the slope calculation would add 
significant processing overhead.  A conservative upper bound can be 
taken by looking at the minimum normal distance of any vertex 
referencing a given texture in a frame.  This will overestimate the 
required textures by 2x or so and still leave a big hit when the top 
mip level loads for big textures, but it can allow giant cathedral 
style scenes to render without swapping.

Clever programmers can always work harder to overcome obstacles, but in 
this case, there is a clear hardware solution that gives better 
performance than anything 